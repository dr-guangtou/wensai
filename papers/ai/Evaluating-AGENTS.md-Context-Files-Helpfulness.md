---
title: "Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?"
authors:
  - Thibaud Gloaguen
  - ETH SRI
source: arXiv:2602.11988v1
date_published: 2026-02-12
date_read: 2026-02-22
type: paper_summary
category: ai
venue: ICML (Machine Learning)
tags:
  - coding-agents
  - llm
  - agents.md
  - context-files
  - software-engineering
  - swebench
  - evaluation
---

# Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?

## Paper Metadata

| Field | Value |
|-------|-------|
| **Title** | Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents? |
| **arXiv ID** | 2602.11988v1 |
| **Published** | February 12, 2026 |
| **Authors** | Thibaud Gloaguen et al. (ETH SRI) |
| **Venue** | ICML (Machine Learning) |
| **Code** | https://github.com/eth-sri/agentbench |

---

## tl;dr (Key Finding)

**Surprising result:** Context files (AGENTS.md, CLAUDE.md) tend to *reduce* coding agent performance while *increasing* costs by >20%. 

- **LLM-generated context files:** Decrease success rates by ~3%
- **Developer-written context files:** Only marginally improve success (+4% vs no context)
- **Recommendation:** Keep context files minimal—only specific tooling requirements, not broad guidelines

---

## Background: What are Context Files?

Context files (AGENTS.md, CLAUDE.md, etc.) are README-style documents targeting AI coding agents. They're designed to help agents:
- Navigate repositories more efficiently
- Run build/test commands correctly
- Adhere to style guides and design patterns
- Solve tasks more successfully

**Adoption:** Over 60,000 public GitHub repositories include context files (as of the paper's writing).

**Industry recommendations:** Major players (Anthropic, OpenAI) strongly encourage using context files to adapt agents to repositories.

---

## The Research Question

Despite widespread adoption, **no rigorous study** had evaluated whether context files actually improve coding agent performance on real-world tasks.

**Challenges for evaluation:**
1. Context files are recent (formalized August 2025)
2. Most repositories don't have them
3. Need for benchmarks with developer-committed context files

---

## Methodology

### Two Evaluation Settings

| Setting | Benchmark | Context File Source |
|---------|-----------|---------------------|
| **Setting 1** | SWE-bench Lite (popular repos) | LLM-generated (following vendor recommendations) |
| **Setting 2** | AGENTbench (new benchmark, 138 instances) | Developer-provided (repos with committed AGENTS.md) |

### AGENTbench: New Benchmark

The authors created **AGENTbench**—a SWE-bench-like benchmark specifically for evaluating context files:

- **138 instances** from 12 repositories
- **Real GitHub issues:** Bug-fixing and feature addition tasks
- **Niche repositories:** Recent/smaller projects with developer-written context files
- **Standardized task descriptions:** Generated by LLM agent to ensure clarity
- **Generated unit tests:** Since niche repos often lack comprehensive tests

### Three Evaluation Conditions

For each task, agents were tested under:

1. **No context file** (baseline)
2. **LLM-generated context file** (following agent-developer recommendations)
3. **Developer-provided context file** (from repository)

---

## Main Results

### Quantitative Findings

| Context Type | Performance vs Baseline | Cost Impact |
|--------------|------------------------|-------------|
| **No context** | Baseline | Baseline |
| **LLM-generated** | **-3%** success rate | **+20%** inference cost |
| **Developer-written** | **+4%** success rate | **+20%** inference cost |

**Key observations:**
- LLM-generated context files *hurt* performance
- Developer-written files help only marginally
- Both approaches significantly increase token usage (>20%)

### Robustness

These findings hold across:
- Different coding agents
- Different underlying LLMs
- Different prompts for generating context files

---

## Behavioral Analysis

The authors analyzed agent traces to understand *why* context files affect performance:

### Increased Exploration

Context files encourage agents to:
- Perform more thorough testing
- Traverse more files
- Engage in more reasoning steps

### Agents Respect Instructions

Agents *do* follow context file instructions—they're not being ignored. The problem is that **more instructions create more constraints**, making tasks harder.

### The "Unnecessary Requirements" Problem

Context files often include:
- ✓ **Helpful:** Specific tooling commands (`pytest --cov`)
- ✗ **Harmful:** General guidelines ("always use functional programming", "never use recursion")

These unnecessary constraints:
- Limit the agent's problem-solving flexibility
- Add cognitive overhead
- Increase token consumption without value

---

## Implications & Recommendations

### For Developers Writing Context Files

**DO:**
- ✅ Include minimal, specific requirements
- ✅ Document specific tooling (test commands, linting setup)
- ✅ Specify environment setup that can't be inferred

**DON'T:**
- ❌ Include broad coding philosophy or style preferences
- ❌ Add constraints that limit solution approaches
- ❌ Document general best practices (agents already know these)

**The paper's recommendation:**
> "Human-written context files should describe only minimal requirements."

### For Agent Vendors

**Current recommendation (from vendors):** Generate context files automatically using `/init` commands.

**This paper's finding:** LLM-generated context files *decrease* performance. Vendors should reconsider this recommendation until generation quality improves.

### Research Directions

The authors hope their evaluation framework will help:
- Improve LLM-generated context file quality
- Develop better context file generation techniques
- Create more effective ways to extract repository-specific knowledge

---

## Critical Analysis

### Strengths of This Study

1. **First rigorous evaluation:** Moves beyond anecdotal evidence
2. **Novel benchmark (AGENTbench):** Addresses the lack of repos with context files
3. **Complementary settings:** Both popular repos (SWE-bench) and niche repos (AGENTbench)
4. **Behavioral analysis:** Not just metrics—understands *why* results occur
5. **Robust findings:** Consistent across models and prompts

### Limitations

1. **Niche repositories:** AGENTbench focuses on smaller repos—findings may not generalize to large, complex codebases
2. **Task types:** Primarily bug-fixing and feature addition—may not apply to refactoring, optimization, etc.
3. **Short-term evaluation:** Doesn't assess long-term benefits of context files
4. **Quality variance:** Developer-written files vary widely in quality

### Open Questions

- Would *high-quality* developer-written context files perform better?
- Are there specific types of repositories where context files help more?
- Could context files help with cross-file understanding in very large codebases?
- How do context files interact with longer context windows in newer models?

---

## Connection to My Workflow (Dr. Guangtou's Notes)

**Relevant to my setup:**
- I have an AGENTS.md in my workspace
- I use Claude Code and other coding agents regularly
- I have specific conventions (snake_case, uv for Python, etc.)

**What I should consider:**

1. **Audit my AGENTS.md:** Remove general advice that Claude already knows
   - ❌ Remove: "Use snake_case" (Claude knows this)
   - ❌ Remove: "Be careful with file deletions" (Claude already is)
   - ✅ Keep: "Use `uv` exclusively (never pip)" (specific tooling)
   - ✅ Keep: Directory-specific guidance (e.g., "Never touch ~/Dropbox/Personal")

2. **Focus on *repository-specific* knowledge:**
   - What would a new developer need to know?
   - What tools/commands are unique to this setup?
   - What are the "gotchas" that aren't obvious from the code?

3. **Keep it minimal:**
   - The paper suggests less is more
   - Long context files increase cost without proportional benefit

---

## Key Takeaways

| Finding | Impact |
|---------|--------|
| Context files increase costs by >20% | Higher token usage without proportional benefit |
| LLM-generated files hurt performance | Vendors' `/init` recommendations may be premature |
| Developer files help only marginally | +4% improvement may not justify the cost |
| Agents respect instructions | The problem is *too many* constraints, not ignored instructions |
| Minimal requirements work best | Focus on specific tooling, not general philosophy |

---

## Related Work References

The paper cites:
- **SWE-bench** (Jimenez et al., 2024): The gold standard for coding agent evaluation
- **Coding agents** (Yang et al., 2024; Wang et al., 2025): LLM-based autonomous coding systems
- **Context file surveys** (Chatlatanagulchai et al., 2025; Mohsenimofidi et al., 2025): Descriptive studies of context file contents
- **Industry adoption** (Sarkar, 2025): Rapid adoption of coding agents in software engineering

---

## Citation

```bibtex
@article{gloaguen2026evaluating,
  title={Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?},
  author={Gloaguen, Thibaud and others},
  journal={arXiv preprint arXiv:2602.11988},
  year={2026}
}
```

---

## Personal Action Items

- [ ] Review my AGENTS.md and identify general vs. specific guidance
- [ ] Remove redundant advice that Claude already knows
- [ ] Add explicit guidance on:
  - [ ] The `uv` requirement (specific tooling)
  - [ ] Protected directories (Dropbox/Personal)
  - [ ] Obsidian vault locations and CLI usage
- [ ] Measure: Does shorter AGENTS.md improve Claude Code performance?
