---
title: "Writing - Letter to a PhD student"
source: "https://davidbessis.substack.com/p/letter-to-a-phd-student"
author:
  - "[[David Bessis]]"
published: 2026-02-17
created: 2026-02-21
description: "What's the point of intellectual work, if AGI is around the corner?"
tags:
  - "clippings"
---
### What's the point of intellectual work, if AGI is around the corner?

![](https://substackcdn.com/image/fetch/$s_!UPhs!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa072f047-dc5d-4746-a049-d469214fb7e5_1567x562.png)

*One morning about a week ago, I opened my direct messages tab on X and found this inbound request:*

> I am someone finishing up a PhD in *\[a subject that isn’t pure math\]*. How should I cope with the claims that agi is here in 1-3 years and that all intellectual labor will be done. It seems many are doing great research right now but is it all for naught?

*Scrolling my timeline, I found a viral post (falsely) claiming that “the top physicists already surrendered” to AI superiority. All this took place while I was on a train to Paris-Saclay University, to speak on “math in the age of AI” at a workshop for machine-learning graduate students.*

*I returned to my direct messages tab, thought about a quick response, then realized that it deserved a much longer one. I continued ruminating throughout the day and, the next morning, I reached out to ask permission to write a long-form public response.*

Dear Student,

I should start with a disclaimer—I don’t know much about your domain, and my reply will draw heavily from my own experience (an N=1 sample). But I suspect this is part of what you’re looking for, since you reached out to an actual human.

An easy answer would be that true AGI is unlikely to be here in 1-3 years. But that would miss the fundamental reality that *some incomplete form* of superhuman AI is already here, already disrupting the value chain of intellectual creation.

A friend of mine, an excellent mathematician who won several high-profile prizes, recently confessed to me that even she was a bit afraid—not for herself, as her career is firmly established, but for her students.

I do not think *“all intellectual labor will be done”* any time soon. But I do think that we are entering a mass extinction event where legacy career strategies for intellectuals will become demonetized. This is how I am reinterpreting your question—not as one about coping, which never works, but as one about surviving and thriving in a radically new environment.

The problem is, nobody knows what the world will look like 20 years from now. I don’t have a clear idea either, and cannot give you a prepackaged answer. What I can do, however, is share my perspective on the true nature and core value creation of intellectual work.

What’s the point of all that? Is it ever a good idea to embark on an academic career?

These questions predate the rise of LLMs. By some accident of history, the recent generations of academics could afford to mostly dodge them. This is no longer the case and AI is only the last straw.

I do not think these questions have generic answers. They are contextual, from one person to another, from one moment of your life to another.

Selfish answers are entirely OK when you’re a PhD student or postdoc: *“The point is right now I’d rather do that than finding a real job. I’m learning tons of cool stuff and it’s fun to cosplay as a career intellectual. I do feel like a fraud, but I’m curious to see if I can outgrow this feeling.”*

I actually think this is the best answer for most PhD students. This should be the time of peak impostor syndrome, and if that’s not the case, then it’s usually a bad sign.

Impostor syndrome, in itself, is a good reason to not get distracted by the AGI discourse, not that there isn’t something to it, but that you’re at the point in your trajectory where you’re likely to vastly underestimate your true capabilities. This skews your perceptions. Eg, you’ve just framed AI as a threat with which you have to cope, rather than an opportunity, whereas one could rationally argue that you’re much better positioned to benefit from it than your PhD advisor and all the academics you look up to.

My go-to career advice for PhD students and postdocs is to never ever let fear discourage you, and to never set any limits to what you can accomplish. It’s not that you won’t encounter practical limits in the future—you will—but you currently have no idea when and where and no way of knowing except for trying.

That certainly means that you should complete your PhD. But that doesn’t necessarily mean that you should seek a postdoc and/or a tenure-track position. It might be a good idea or a bad idea, depending on your personal situation, the credible opportunities on the visible horizon, your constraints and priorities, and how you perceive your optionality.

At this point, it’s still OK to be pragmatically selfish. Do you think a few more years of academia will make you stronger and more confident? Do you think that you still have important things to learn and discover? Or are you mostly concerned about the risks to your value on the job market? These are tough, private decisions, and they are hard to get right when impostor syndrome clouds your judgment—but fantasies about long-term prospects shouldn’t serve as excuses.

To me, your question about AGI concerns the next phase, when your motivation should gradually shift from *“will it make me stronger?”* to *“which value am I really bringing to the world?”*

This is the question that made me decide, at 35, to quit my [permanent research-only position](https://en.wikipedia.org/wiki/French_National_Centre_for_Scientific_Research). And it’s the same question that, 15 years later, made me decide to resume full-time intellectual work, as an independent author.

I was never comfortable with the attitude of many of my peers. I remember a conference dinner where a brilliant young mathematician, a few years older than me, declared that that he didn’t think his research had any social value, but was nevertheless extremely happy to have the privilege of focusing full time on it.

At least he was *“doing great research”* (to quote your words).

It is a sad reality that not all tenured academics are doing great research. Before I try to clarify what is at stake, let me go back to the functioning of academia and the unique circumstances that led to the current situation.

University professors like to think that their primary job is to advance science. But except in specific fields like engineering or biomedical sciences, there is very little demand for the science they produce, and academics aren’t *really* hired for that part of the job.

The fabulous expansion of academia in the second half of the 20th century was for a large part a response to the fabulous expansion of undergraduate education, itself driven by the demographic boom and the democratization of college. And the current crisis of academia, which predates LLMs, is primarily explained by the nascent demographic decline and the stagnating demand for higher education.

What about research, in this context?

Sure, academic research seeks to push the frontiers of human understanding. But to a large extent it is a by-product of the academic technostructure, where it serves both as a perk and a career evaluation framework.

The opening lines of David Lodge’s *[Changing Places: A Tale of Two Campuses](https://en.wikipedia.org/wiki/Changing_Places)*, published in 1975, capture the “perk” dimension at its historical peak (back then, international air travel was a rare luxury):

> High, high above the North Pole, on the first day of 1969, two professors of English Literature approached each other at a combined velocity of 1200 miles per hour. They were protected from the thin, cold air by the pressurized cabins of two Boeing 707s, and from the risk of collision by the prudent arrangement of the international air corridors. Although they had never met, the two men were known to each other by name. They were, in fact, in process of exchanging posts for the next six months, and in an age of more leisurely transportation the intersection of their respective routes might have been marked by some interesting human gesture: had they waved, for example, from the decks of two ocean liners crossing in mid-Atlantic, each man simultaneously focusing a telescope, by chance, on the other, with his free hand…

Meanwhile, the “evaluation framework” aspect has gained pre-eminence, creating massively distorted incentives. Non-tenured academics publish to survive, tenured academics publish to maintain their symbolic status—and this goes a long way to explaining the monstrous scientific overproduction and its associated pathologies, from futility to self-plagiarism, from predatory journals to outright fraud.

In pure mathematics, where deciphering a single paper outside of your core expertise can require weeks of effort, the fragmentation of knowledge and comprehension is so profound that becoming familiar with two distinct micro-domains with cross-applicability can grant you a career-long monopoly.

It’s never entirely clear whether overspecialization and technical inaccessibility are bugs or features. On the one hand, these seem to be unavoidable consequences of the unstoppable expansion of knowledge. On the other hand, they are so miraculously aligned with the interests of the stakeholders that you can’t help wondering.

This aligns surprisingly well with [Peter Thiel’s mantra](http://csun.edu/~vcact00f/497CapStone/Peter%20Thiel_%20Competition%20Is%20for%20Losers%20-%20WSJ.pdf): *“competition is for losers: if you want to create and capture lasting value, look to build a monopoly.”*

![](https://substackcdn.com/image/fetch/$s_!i7ca!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1c64a913-abf9-40cd-9e7f-1436a6ef50f1_1280x853.jpeg)

The Peabody Library of the Johns Hopkins University

Will AI replace intellectual labor? Well, if intellectual labor consists of scanning gazillions of preprints and data sources, spotting hidden connections, applying known techniques, and writing boilerplate copy, it surely will—and, remarkably, this will suffice to produce original research.

But original research isn’t the same as great research. Contrary to a delusion that many academics have bought into, you can spend your life expanding an obscure corner of the cathedral of knowledge, and still create no meaningful value.

Here’s how [Larry McEnerney](https://en.wikipedia.org/wiki/Lawrence_McEnerney) articulates it in his unforgettable lecture on *[The Craft of Writing Effectively](https://www.youtube.com/watch?v=vtIzMaLkCaM&t=823s)*, a must-watch for anyone with intellectual or creative ambitions:

> Faculty come into my office, and forgive me for the drama, but in my office, there’s two chairs in the writing corner. I have a chair at my desk and then there’s two chairs over here. There’s my chair and then there’s the writer’s chair and next to the writer’s chair is a box of Kleenex. And I’m not kidding.
> 
> Because I have people coming to me saying, *“they’re not accepting my proposal, they’re not accepting my draft.”* I get faculty who come in and say *“they’re not publishing my work.”* And of course there’s Kleenex there because like, you know, careers are depending on it. And sometimes it’s because it’s not clear. And sometimes it’s because it’s not organized. And sometimes it’s because it’s not persuasive. But overwhelmingly it’s because it’s not valuable.
> 
> And the other stuff doesn’t matter. If it’s clear and useless, it’s useless. It’s organized and useless, it’s useless. It’s persuasive and useless, it’s useless.
> 
> That’s the way it is.

This pain was already there and it will only increase. LLMs crush humans on clarity, organization, persuasiveness. They still struggle with rigor but are making steady progress on that front.

Yet the core question of value creation remains intact, and in the foreseeable future it will remain a question for humans. Why are we studying this or that? Are we happy or not with the results? What are we really trying to figure out? How will it help us make sense of the world?

Your job as an intellectual is to own these questions, not to hide behind the smoke screen of technical mastery.

This may be disconcerting, because these aren’t well-posed questions. They are part of the annoying fluff your advisor forces you to add to get your project financed, and you know that the real work takes place downstream, once they have been transcribed into better-posed technical questions. Or does it?

It’s precisely because these questions aren’t well-posed that we need humans to think about them. If the real job of radiologists was to spot tumors on X-rays, then [this 2016 recommendation by Geoff Hinton](https://www.youtube.com/watch?v=2HMPRXstSvQ&t=6s) would have been wiser:

> We should stop training radiologists now. It’s just completely obvious that within five years, deep learning is going to do better than radiologists.

([Ten years later, the US is facing a major shortage of radiologists](https://pubs.rsna.org/doi/10.1148/radiol.232625).)

Or, if you prefer a more thoroughly time-tested example—if the real job of painters was visual accuracy, then academic painter [Paul Delaroche](https://en.wikipedia.org/wiki/Paul_Delaroche) would have been right to declare in 1839, upon seeing a daguerreotype:

> From today, painting is dead.

Interestingly, the quote has no traceable source and it is likely to be apocryphal. In any case, Delaroche peacefully pursued his own creative career until his death in 1856.

I like the photography parallel because it illustrates the nature of our blindspots. From *“photography will kill painting”* to *“AI will kill intellectual work”*, we’re being fed barbarians-vs-luddites tales of technology disruption. But at a deeper level, innovation triggers cascading cognitive disruptions, whose long-term effects are much harder to predict. To someone like Paul Delaroche, the art of Monet, van Gogh, Cézanne, Pollock, or Warhol, wasn’t even the stuff of nightmares—it was plainly *unthinkable*.

To be honest, this framing would have terrified me as a PhD student, when I had no serious scientific ambition, no expectation that my work would ever matter, and profound insecurities about my own capabilities and future. Technical mastery was hard enough, I would have hated to see that rug pulled from under my feet.

But you could see it the other way around: by becoming less dependent on technical mastery, research could become way more exciting.

I don’t think you should make too much of it in the short term. In any case, you still have to operate within the existing framework, and technical mastery still is on the critical path. Your investment won’t be lost, not because you will continue to practice the same skills for the next thirty years, but because they will rewire your brain and elevate your worldview, which you will need to competently interact with whatever technology exists in future. (It’s the same reason why art students continue to study the old masters, and continue to learn portrait techniques and perspective.)

But to guide you over the long term, I do think you should keep two open tabs at the back of your mind and revisit them every year or so, in a sort of mental retreat:

1. *Technology*. You can’t pretend it’s not there. No field will be immune to it, although the penetration and time dynamics will vary a lot. What’s tricky is that this will never be a stable situation. The hardest thing will be to refrain from passing definitive judgments, and forcing yourself to regularly experiment with new tools, new versions and new use cases (this will confront you to difficult explore/exploit dilemmas).
2. *Sincerity/alignment*. Are you really working on stuff that matters? If successful, will your research create meaningful value beyond your peer network? Again, it’s OK to be pragmatic and even a bit cynical in the short term, especially when the institution is imposing impossible demands on you. But you have to be very careful and not let this sincerity gap widen over time—ideally it should shrink at every major step of your career.

It’s much easier to embrace technological change once you have a clear idea of what you’re trying to achieve. There’s always been good and bad reasons to engage in intellectual work, and AI will make the bad reasons infinitely worse.

If the sincerity/alignment aspect feels too abstract or grandiose, here’s a practical proxy: *are you genuinely interested in what you are working on? If you didn’t have to earn a living, and if you didn’t have any social insecurities, would you still be doing that?*

In a way, this isn’t much different from [Rilke’s advice on writing](https://rilkepoetry.com/letters-to-a-young-poet/letter-one/):

> No one can advise and help you, no one. There is only one way. Withdraw into yourself. Explore the reason that bids you write, find out if it has spread out its roots in the very depths of your heart.

I’ve been using this framework for many years and continue to find it extremely helpful. I suspect it touches something fundamental about intellectual creation, which will become even more critical in the age of AI—if someone is ever to hold a durable cognitive edge, [it can only be proprioceptive and attentional](https://davidbessis.substack.com/p/attention-is-all-we-have).

Indeed, your intellectual sincerity is arguably your most unique and defensible cognitive resource. You have to trust your subjectivity, because it is the only place where everything comes together, the official knowledge that AI can already process infinitely faster than you, and your embodied intelligence, your life experience, the things you’ve learnt from the world and cannot yet articulate.

Human creativity takes place at this interface, which is also where it is received and valued by others.

I see no hard reason why machines couldn’t some day emulate our subjective, carnal experience of being alive, and out-human us in every possible way. But this purely speculative question has no relevance to your career prospects, as the current technology cycle is absolutely not about that. It is plausible that the academic job market could become tighter in the coming years, but our urge to better understand the world isn’t going to vanish—and, at its core, this is a human endeavor.

Whether in AI or in any other discipline, the truth is we’ve barely scratched the surface. Technology is progressing at an intimidating pace, but we don’t get to choose and the fundamental question remains what we’re going to do with it.

So if you find your research genuinely exciting, and if you have the opportunity to pursue it in favorable conditions, I see no reason why you should give up.

In particular, I have no doubts that your generation and the generations to come will continue to produce successful intellectuals, in all domains of knowledge, and that their work will continue to be recognized and valued. Some of this work will even be considered revolutionary, even though I have no idea what shape it will take.

In any case, doing what you truly care about is the best way to minimize regret, an essential objective in times of uncertainty.

All the best,  
David.